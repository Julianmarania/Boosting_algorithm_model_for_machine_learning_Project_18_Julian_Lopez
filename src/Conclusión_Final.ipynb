{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusión final "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resultados (Accuracy score) Decision tree model:\n",
    "\n",
    "Score train: (0.7996742671009772,)\n",
    "\n",
    "Score test: 0.7727272727272727\n",
    "\n",
    "### Resultados (Accuracy score) Random Forest model:\n",
    "\n",
    "Score train: (0.8224755700325733,)\n",
    "\n",
    "Score test: 0.7467532467532467\n",
    "\n",
    "### Resultados (Accuracy score) Boosting models:\n",
    "\n",
    "##### Boosting with `GradientBoostingClassifier` and `GridSearchCV`:\n",
    "\n",
    "Score train: (0.8061889250814332,)\n",
    "\n",
    "Score test: 0.7922077922077922\n",
    "\n",
    "##### Boosting with `XGBClassifier` and `GridSearchCV`:\n",
    "\n",
    "Score train: (0.8534201954397395,)\n",
    "\n",
    "Score test: 0.7662337662337663\n",
    "\n",
    "##### Boosting with `XGBClassifier`:\n",
    "\n",
    "Score train: 0.7947882736156352\n",
    "\n",
    "Score test: 0.7987012987012987"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como podemos obaserbar en la parte superior, en los resultados (Accuracy score) de los diferentes modelos, el primer modelo en implementar sobre este conjunto de datos fue el `Decision Tree model` y obtenemos de el unos resultados que podemos considerar buenos, aunque mejorables, tras la optimización mediante hypreparametros no obserbamos overfiting, esto quiere decir que el modelo aprendew y generaliza de una manera bastante pareja.\n",
    "\n",
    "El segundo modelo en implementar ha sido `Random Forest model`, los resultados de este han sido un poco peor y aunque hemos logrado bajar mucho el overfitin principal mediante el Feature selection y la optimización mediante hypreparametros, no conseguimos mejorar los resultados del primer modelo y aunque este aprende mejor que el anterior, a la hora de generalizar nos osfrece unos resultados mas bajos.  \n",
    "\n",
    "Para implementar el modelo de `Boosting`, he descubierto que habia varias formas de hacerlo, con librerias distintaas y diferentes maneras tanto de entrenar el modelo como de optimizarlo con la hyperparametrización. De las tres formulas que he probado el mejor resultado lo hemos conseguido a través de un modelo Boosting con la libreria `xgboost`. El modelo `XGBClassifier` nos ofrece no solo los mejores resultados de todos los modelos probados, tanto de Boosting como del resto de modelos implementados. Con un accuracy score sin overfiting en el que el modelo aprende y generaliza y es capaz de predecir un 80% de la información ofrecida en el conjunto de datos. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
